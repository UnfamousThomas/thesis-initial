{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Thesis Documentation","text":"<p>This project began as a solution to a problem I encountered during my Computer Science Bachelor's thesis. Its main goal is to enable game server management on Kubernetes, allowing deletion blocking without adding complex logic to the game servers themselves. Additionally, it supports custom autoscaling decisions through webhooks.</p>"},{"location":"#about-the-documentation","title":"About the Documentation","text":"<p>The structure is kept intentionally simple:</p> <ul> <li>Left sidebar: Navigate between different pages of the documentation.</li> <li>Right sidebar: View the subsections of the current page.</li> </ul> <p>Each page covers either: - A core part of the project, - Usage instructions, or - An architectural decision, including its consequences and reasoning.</p>"},{"location":"architecture/","title":"Architecture","text":"<p>This page provides an overview of the general architecture of the project, explaining how the custom resources work together to manage game servers in a Kubernetes environment.</p>"},{"location":"architecture/#custom-resources","title":"Custom Resources","text":"<p>The project implements four main custom resources to manage game servers:</p> <ul> <li>Server</li> <li>Fleet</li> <li>GameType</li> <li>GameAutoscaler</li> </ul>"},{"location":"architecture/#resource-overview","title":"Resource Overview","text":"<ul> <li>Server: A Server is a wrapper for a Kubernetes pod with added logic, typically through a sidecar.</li> <li>Fleet: A Fleet manages the creation and deletion of multiple servers, ensuring scaling and resource management for a group of servers.</li> <li>GameType: A GameType manages one or two fleets and is responsible for updating the configuration of servers, enabling gradual upgrades.</li> <li>GameAutoscaler: A GameAutoscaler triggers scaling actions based on external logic, calling a webhook implemented by the user to adjust the replica count of the GameType.</li> </ul>"},{"location":"architecture/#resource-relationships","title":"Resource Relationships","text":"<p>The resources in this project are closely interconnected. Here's how they relate:</p> <ul> <li>GameAutoscaler scales a GameType based on external inputs (via webhook).</li> <li>GameType creates and updates Fleets based on its configuration.</li> <li>Fleets manage the creation and deletion of Servers.</li> <li>Servers manage individual Pods.</li> </ul> <p>This relationship can be visualized in the following diagram:</p> <p></p>"},{"location":"architecture/#server-deletion-blocking","title":"Server Deletion Blocking","text":"<p>The blocking of server deletion is a critical aspect of this system. It ensures that a server is not deleted until all conditions for safe deletion are met.</p> <p>This is achieved through the use of finalizers. When a new Server object is created, the controller automatically adds a finalizer. The finalizer ensures that the server cannot be deleted until the associated sidecar reports that deletion is allowed.</p> <p>The process is as follows:</p> <ol> <li>A finalizer is added to the Server object upon creation.</li> <li>The Sidecar (detailed in Sidecar) monitors the server and reports when deletion is allowed.</li> <li>Only once the sidecar confirms that deletion is allowed will the finalizer be removed, and the Server can be deleted.</li> </ol> <p>The flow of communication between components can be seen in the following diagram:</p> <p></p>"},{"location":"autoscaler/","title":"GameAutoscaler","text":"<p>The GameAutoscaler is a resource that periodically triggers a webhook to adjust the replica count of a GameType. It sends requests to the defined webhook endpoint, and based on the response, it adjusts the number of replicas.</p>"},{"location":"autoscaler/#manifest","title":"Manifest","text":"<p>The manifest for a GameAutoscaler object looks like this: <pre><code>apiVersion: network.unfamousthomas.me/v1alpha1\nkind: GameAutoscaler\nmetadata:\n  name: gameautoscaler-sample\nspec:\n  gameName: gametype-sample # (1)!\n  policy:\n    type: webhook # (2)! \n    webhook: # (3)!\n      url: \"http://localhost:2411\" # (4)!\n      path: \"/scale\" # (5)!\n      service: # (6)!\n        name: someService123 \n        namespace: default\n        port: 8080\n  sync:\n    type: fixedinterval # (7)!\n    interval: 10m # (8)!\n</code></pre></p> <ol> <li>Gametype gametype-sample has to exist.</li> <li>Currently only support Webhook.</li> <li>Webhook specifications, either path and url OR service need to be defined.</li> <li>The url to send the request to. Combined with path if provided.</li> <li>Path to send the request to. Combined with url.</li> <li>Service to send the request to.</li> <li>Currently only supports fixedinterval.</li> <li>How often to send the webhook.</li> </ol>"},{"location":"autoscaler/#basic-concept","title":"Basic Concept","text":"<p>The GameAutoscaler object simplifies the autoscaling of GameType resources by using a webhook. The key fields to configure are:</p> <ul> <li><code>gameName</code>: The name of the (already existing) GameType that needs to be scaled</li> <li><code>policy.webhook</code>: The configuration of the webhook</li> <li><code>sync</code>: Defines how often and how the webhook should be triggered. </li> </ul>"},{"location":"autoscaler/#request-jsons","title":"Request JSONs","text":"<p>When the GameAutoscaler triggers the webhook, it sends a request in the following format: <pre><code>{\n  \"game_name\": \"gametype-sample\",\n  \"current_replicas\": 5\n}\n</code></pre></p>"},{"location":"autoscaler/#response-jsons","title":"Response JSONs","text":"<p>The webhook should respond with a JSON object indicating whether scaling is required and the desired replica count:</p> <p><pre><code>{\n  \"scale\": true,\n  \"desired_replicas\": 10,\n}\n</code></pre> If the webhook returns the above, the GameAutoscaler will update the GameType to have 10 replicas.</p>"},{"location":"autoscaler/#go-structs","title":"Go Structs","text":"<p>For those interested in implementing the webhook in Go, here are the Go structs representing the request and response formats:</p> <pre><code>type AutoscaleRequest struct {\n    GameName        string `json:\"game_name\"`\n    CurrentReplicas int    `json:\"current_replicas\"`\n}\n\ntype AutoscaleResponse struct {\n    Scale           bool `json:\"scale\"`\n    DesiredReplicas int  `json:\"desired_replicas\"`\n}\n</code></pre>"},{"location":"fleet/","title":"Fleet","text":"<p>A Fleet represents a group of servers managed together. Fleets simplify the process of scaling the number of servers up or down, based on the needs of the game or application. They act as a container for managing the lifecycle and scaling rules of multiple servers.</p>"},{"location":"fleet/#manifest","title":"Manifest","text":"<p>The manifest for a Fleet object looks like this:</p> <pre><code>apiVersion: network.unfamousthomas.me/v1alpha1\nkind: Fleet\nmetadata:\n  name: fleet-sample\n  labels:\n    some-label: value1 # (5)!\nspec:\n  scaling:\n    replicas: 3 # (1)!\n    prioritizeAllowed: true # (2)!\n    agePriority: oldest_first # (3)!\n  spec: # (4)!\n    timeout: 5m\n    allowForceDelete: false\n    pod:\n      containers:\n        - name: example-container\n          image: nginx:latest\n          ports:\n            - containerPort: 80\n              protocol: TCP\n          resources:\n            limits:\n              cpu: \"500m\"\n              memory: \"256Mi\"\n            requests:\n              cpu: \"250m\"\n              memory: \"128Mi\"\n</code></pre> <ol> <li>The number of servers that should exist in this fleet.</li> <li>Whether to delete allowed servers first during downscaling.</li> <li>Whether to delete oldest servers first or newest (<code>newest_first</code>).</li> <li>The spec for the servers in this fleet (same as a Server object spec). Note: When this is changed it applies to new servers created  but not to old ones.</li> <li>Labels are copied down to the Server resource.</li> </ol>"},{"location":"fleet/#scaling-behaviour","title":"Scaling Behaviour","text":"<ul> <li>Replicas: The <code>replicas</code> field defines how many server instances should exist in the fleet at any time. This field ensures that the fleet always maintains the desired number of servers.</li> <li>PrioritizeAllowed: The <code>prioritizeAllowed</code> field determines whether servers that have deletion allowed should be deleted first during a downscale event. If <code>true</code>, servers that are marked as allowed for deletion will be removed first to maintain the specified number of replicas.</li> <li>AgePriority: The <code>agePriority</code> fields determines the order in which servers are deleted when downscaling. By default, <code>oldest_first</code> will remove the oldest servers first. If set to newest_first, the most recently created servers will be deleted first. You can add additional priorities here as needed.  </li> </ul> <p>Simply put, this just acts as a simple container for multiple servers.</p>"},{"location":"fleet/#default-behaviour","title":"Default Behaviour","text":"<ul> <li>Replicas: If not specified, the controller will default to creating 1 replica of the server</li> <li>PrioritizeAllowed: If not specified, it defaults to <code>true</code>, meaning the controller will prioritize deleting allowed servers during downscaling.</li> <li>AgePriority: If not specified, the controller will default to <code>oldest_first</code>, ensuring that the oldest servers are removed first during downscaling.</li> </ul>"},{"location":"fleet/#spec-inheritance","title":"Spec Inheritance","text":"<p>The <code>spec.spec</code> field inside the Fleet manifest is identical to the spec used in the Server object. This means that each server created by a fleet will inherit its configuration from this spec, including container settings, resource requests, and limits.</p> <p>For more information on the Server spec, see the Server documentation.</p>"},{"location":"gametype/","title":"GameType","text":"<p>A GameType represents a scalable container for one or more fleets, enabling the management and upgrading of server versions. It serves as the central unit for scaling and versioning of game server deployments.</p>"},{"location":"gametype/#manifest","title":"Manifest","text":"<p>The manifest for a GameType object looks like this:</p> <pre><code>apiVersion: network.unfamousthomas.me/v1alpha1\nkind: GameType\nmetadata:\n  name: gametype-sample\nspec:\n  fleetSpec: # (1)! The specification for the fleet associated with this GameType\n    scaling:\n      replicas: 3 # (2)!\n      prioritizeAllowed: true # (3)!\n      agePriority: oldest_first # (4)!\n    spec: # (5)!\n      timeout: 5m\n      allowForceDelete: false\n      pod:\n        containers:\n          - name: example-container\n            image: nginx:latest\n            ports:\n              - containerPort: 80\n                protocol: TCP\n            resources:\n              limits:\n                cpu: \"500m\"\n                memory: \"256Mi\"\n              requests:\n                cpu: \"250m\"\n                memory: \"128Mi\"\n</code></pre> <ol> <li>The specification for the fleet associated with this GameType.</li> <li>The number of replicas (servers) to maintain.</li> <li>Whether to delete allowed servers first when downscaling.</li> <li>Whether to prioritize deleting oldest or newest servers when scaling down.</li> <li>The spec for the servers within this fleet (same as a Server object spec).</li> </ol>"},{"location":"gametype/#purpose","title":"Purpose","text":"<p>The GameType object currently acts as a wrapper for 1-2 fleets. While its manifest closely mirrors the Fleet object, it provides the additional role of handling multiple fleet versions. This allows for gradual upgrades or changes in server configurations, with the flexibility to roll out new fleet versions in a controlled manner.</p>"},{"location":"gametype/#upgrade-process","title":"Upgrade Process","text":"<p>When the pod spec (the configuration of the containers) for the servers changes, the GameType initiates the following process: * Create New Fleet: A new fleet is created with the same number of replicas and the updated spec.</p> <ul> <li> <p>Gradual Upgrade:  The old fleet is gradually removed according to the server deletion rules set in the fleet (e.g., prioritizing allowed deletions or age-based deletions).</p> </li> <li> <p>Trigger Old Fleet Deletion: Once the new fleet is running and stable, the old fleet is triggered for deletion, again adhering to the server's deletion policies.</p> </li> </ul> <p>This ensures a smooth upgrade process, minimizing downtime and adhering to the configured server policies.</p>"},{"location":"gametype/#future-changes","title":"Future Changes","text":"<p>The GameType spec may evolve in the future to provide more advanced scaling or versioning capabilities. However, for now, it functions as a simple container for one or two fleets, with the primary goal of supporting controlled upgrades and scaling of game servers.</p>"},{"location":"server/","title":"Server","text":"<p>A server is the core resource in this project, serving as the foundation for managing game server instances in Kubernetes.</p> <p>It has a 1-to-1 relationship with a pod, meaning each server corresponds to a single pod. The server acts as a simple wrapper around the pod, with an additional sidecar inserted to manage specialized functionality (such as lifecycle control and custom metrics). For more information about the sidecar, please see the Sidecar documentation.</p>"},{"location":"server/#manifest","title":"Manifest","text":"<p>The manifest for a Server object looks like this:</p> <pre><code>apiVersion: network.unfamousthomas.me/v1alpha1\nkind: Server\nmetadata:\n  labels:\n    someLabel: example-label # (1)!\n  name: server-sample\nspec:\n  timeout: 5m # (2)!\n  allowForceDelete: false # (3)!\n  pod:\n    containers:\n      - name: example-container\n        image: nginx:latest\n        ports:\n          - containerPort: 80\n            protocol: TCP\n        resources:\n          limits:\n            cpu: \"500m\"\n            memory: \"256Mi\"\n          requests:\n            cpu: \"250m\"\n            memory: \"128Mi\"\n</code></pre> <ol> <li>Labels are copied down to the pod.</li> <li>Time the controller waits before allowing a forced deletion of the server.</li> <li>Determines if the controller can forcefully delete the server without permission. This would mean that it will not ask the sidecar for permission.</li> </ol> <p>The <code>spec.pod</code> sub-object follows the typical Kubernetes pod spec format, allowing you to define multiple containers, volumes, and other pod configurations as necessary.</p>"},{"location":"server/#special-environment-variables","title":"Special Environment Variables","text":"<p>For ease of development and to provide useful context inside containers, the following environment variables are automatically introduced to all containers:</p> <ul> <li><code>CONTAINER_IMAGE</code> - The image of the current container</li> <li><code>SERVER_NAME</code> - The name of the server object</li> <li><code>FLEET_NAME</code> - The name of the parent fleet object (if applicable)</li> <li><code>GAME_NAME</code> - The name of the parent game object (if applicable)</li> <li><code>POD_IP</code> - The IP address of the pod</li> <li><code>NODE_NAME</code> - The name of the node where the pod is running</li> </ul>"},{"location":"server/#server-management","title":"Server Management","text":"<ul> <li> <p>Duration: The <code>duration</code> field determines how long the controller will wait before permitting a forced deletion of the server. This allows you to implement a delay before the server is deleted to prevent premature shutdowns.</p> </li> <li> <p>AllowForceDelete: The <code>allowForceDelete</code> field, when set to <code>true</code>, instructs the controller to delete the server without waiting for user permission. If set to <code>false</code>, the server will only be deleted after a manual approval.</p> </li> </ul> <p>By setting these fields, you can fine-tune how the server lifecycle is managed within your Kubernetes environment.</p>"},{"location":"server/#tips-and-considerations","title":"Tips and Considerations","text":"<ul> <li>Resource Allocation: It\u2019s crucial to define the <code>cpu</code> and <code>memory</code> limits and requests according to the expected usage of the server to avoid performance issues.</li> </ul>"},{"location":"service/","title":"Service","text":"<p>The service provides a quality-of-life feature within the operator to simplify working with custom resources programmatically.</p>"},{"location":"service/#purpose","title":"Purpose","text":"<p>Creating custom resources in Kubernetes programmatically usually involves using the Kubernetes API directly. This often requires setting up ClusterRoles and other complex configurations. The goal of this service is to simplify the process by providing an easy-to-use API that handles the necessary configurations automatically.</p>"},{"location":"service/#api-routes","title":"API Routes","text":"<p>The service currently provides the following API routes for managing resources:</p> Method Route Description <code>POST</code> <code>/server</code> Create a new server. <code>DELETE</code> <code>/server</code> Delete an existing server. <code>POST</code> <code>/server/pod/labels</code> Add labels to the server's pod. <code>DELETE</code> <code>/server/pod/labels</code> Remove labels from the server's pod. <code>POST</code> <code>/fleet</code> Create a new fleet. <code>DELETE</code> <code>/fleet</code> Remove an existing fleet. <code>POST</code> <code>/scaler</code> Create a new autoscaler. <code>DELETE</code> <code>/scaler</code> Delete an existing autoscaler. <p>Additional routes (e.g., <code>GET /server</code>) are planned for future improvements.</p>"},{"location":"service/#json-structures","title":"JSON Structures","text":"<p>The JSON structures for most API calls are similar, with differences primarily in the specifications for each resource. Below are the JSON formats for creating and deleting servers and fleets.</p>"},{"location":"service/#create-server","title":"Create Server","text":"<p>The JSON for creating a new server is structured as follows:</p> <pre><code>{\n  \"server\": {\n    \"metadata\": {\n      \"name\": \"server\",\n      \"namespace\": \"default\",\n      \"labels\": {\n        \"label1\": \"value1\"\n      }\n    },\n    \"spec\": {\n      \"timeout\": \"5m\",\n      \"allowForceDelete\": false,\n      \"pod\": {\n        \"containers\": [\n          {\n            \"name\": \"game-server\",\n            \"image\": \"nginx:latest\"\n          }\n        ]\n      }\n    }\n  }\n}\n</code></pre> <p>This JSON is similar to the manifest defined in the Server section.</p>"},{"location":"service/#create-fleet","title":"Create Fleet","text":"<p>A fleet's JSON structure follows this pattern:</p> <pre><code>{\n  \"fleet\": {\n    \"metadata\": {\n      \"name\": \"fleet\",\n      \"namespace\": \"default\",\n      \"labels\": {\n        \"label1\": \"value1\"\n      }\n    },\n    \"spec\": {\n      \"server\": {\n        \"timeout\": \"5m\",\n        \"allowForceDelete\": false,\n        \"pod\": {\n          \"containers\": [\n            {\n              \"name\": \"game-server\",\n              \"image\": \"nginx:latest\"\n            }\n          ]\n        }\n      },\n      \"scaling\": {\n        \"replicas\": 3,\n        \"prioritizeAllowed\": true,\n        \"agePriority\": \"oldest_first\"\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"service/#delete-resource","title":"Delete Resource","text":"<p>To delete a resource (e.g., server, fleet, scaler), the request should include the following JSON:</p> <pre><code>{\n  \"metadata\": {\n    \"name\": \"server\",\n    \"namespace\": \"default\"\n  },\n  \"force\": false\n}\n</code></pre> <ul> <li>The <code>force</code> field determines whether the resource should be deleted even if dependent resources exist. The default value is <code>false</code> and has no effect on GameAutoscalers.</li> </ul>"},{"location":"service/#add-pod-labels","title":"Add Pod Labels","text":"<p>To add or update labels for a server's pod:</p> <pre><code>{\n  \"metadata\": {\n    \"name\": \"server\",\n    \"namespace\": \"default\",\n    \"labels\": {\n      \"label1\": \"val1\",\n      \"label2\": \"val2\"\n    }\n  }\n}\n</code></pre> <ul> <li>Note: This will overwrite any existing labels with the same key.</li> </ul>"},{"location":"service/#remove-pod-label","title":"Remove Pod Label","text":"<p>To remove a specific label from a server\u2019s pod:</p> <pre><code>{\n  \"metadata\": {\n    \"name\": \"server\",\n    \"namespace\": \"default\"\n  },\n  \"label\": \"label1\"\n}\n</code></pre> <p>This will remove the label with the key <code>label1</code>.</p>"},{"location":"service/#future-enhancements","title":"Future Enhancements","text":"<p>In the future, we plan to add more routes, such as <code>GET /server</code>, to allow for more flexible management of game servers and related resources. Ideally, a dedicated API spec will also be setup for easier understanding of the documentation.</p>"},{"location":"sidecar/","title":"Sidecar","text":"<p>The sidecar pattern is widely used in Kubernetes. It typically involves running a secondary container alongside the main application container to provide additional functionality. You can read more about the pattern here.</p> <p>In this project, the sidecar acts as a communication bridge between the controller and the game server.</p> <p>The sidecar exposes a simple REST API with a few routes that the server and controller interact with.</p>"},{"location":"sidecar/#routes","title":"Routes","text":"<p>The sidecar provides the following endpoints:</p> <ul> <li><code>GET /allow_delete</code></li> <li><code>POST /allow_delete</code></li> <li><code>GET /shutdown</code></li> <li><code>POST /shutdown</code></li> <li><code>/health</code></li> </ul>"},{"location":"sidecar/#allow-delete","title":"Allow Delete","text":"<p>The <code>allow_delete</code> routes are used to check or set whether the server is allowed to be deleted.</p> <ul> <li><code>GET /allow_delete</code> \u2014 Retrieve whether deletion is allowed.</li> <li><code>POST /allow_delete</code> \u2014 Set whether deletion is allowed.</li> </ul> <p>Both the GET and POST methods operate on the same internal boolean value.</p>"},{"location":"sidecar/#request-format","title":"Request Format","text":"<p>JSON Example: <pre><code>{\n  \"allowed\": true\n}\n</code></pre> Go Struct Example: <pre><code>type DeleteRequest struct {\n    Allowed bool `json:\"allowed\"`\n}\n</code></pre></p> <p>The boolean value controls whether the server considers itself safe to delete. * The controller usually issues GET requests to check the status. * The game server typically issues POST requests to update the status.</p> <p>Note:</p> <p>Be cautious when toggling the <code>allowed</code> state back to <code>false</code>. If the controller detects <code>allowed = true</code>, it may already begin the deletion process, even if you immediately set it back to <code>false</code>.</p>"},{"location":"sidecar/#shutdown","title":"Shutdown","text":"<p>The <code>shutdown</code> routes indicate whether a shutdown has been requested for the server.</p> <ul> <li><code>GET /shutdown</code> \u2014 Retrieve whether shutdown has been requested.</li> <li><code>POST /shutdown</code> \u2014 Set whether shutdown has been requested.</li> </ul> <p>Similar to <code>allow_delete</code>, these routes operate on an internal boolean value.</p>"},{"location":"sidecar/#request-format_1","title":"Request Format","text":"<p>JSON Example: <pre><code>{\n  \"shutdown\": true,\n}\n</code></pre> Go Struct Example: <pre><code>type ShutdownRequest struct {\n    Shutdown bool `json:\"shutdown\"`\n}\n</code></pre> The controller sets this flag once it detects a deletion timestamp on the <code>Server</code> object. The game server can poll this value to detect when a shutdown has been requested and gracefully handle it.</p> <p>The communication workflow can be viewed here: </p>"},{"location":"started/","title":"Getting Started","text":"<p>Currently, getting started is a bit complicated due to the images only being hosted on the Github Repository and the helm chart not existing as a directly pullable chart.</p>"},{"location":"started/#requirements","title":"Requirements","text":"<ul> <li>Kubernetes Cluster</li> <li>Github Account</li> <li>git installed on a machine with the cluster accessible</li> </ul>"},{"location":"started/#setup-the-github-secret-in-kubernetes","title":"Setup the github secret in Kubernetes","text":""},{"location":"started/#personal-access-token","title":"Personal Access Token","text":"<p>For this, you need to first setup a Github Personal Access Token with read:packagages permission. This can be done on the  Personal Access Tokens page.</p>"},{"location":"started/#kubernetes-secret","title":"Kubernetes Secret","text":"<p>After that, you need to setup the token as a secret in Kubernetes, you can do that with: <pre><code>kubectl create secret docker-registry ghcr-secret \\\n  --docker-server=ghcr.io \\\n  --docker-username=&lt;github-username&gt; \\\n  --docker-password=&lt;github-token&gt; \\\n  --docker-email=&lt;email-address&gt;\n</code></pre> Note the name you set for this secret, and that it is available in the same namespace where you want to install the controller to.</p>"},{"location":"started/#installing-the-charts","title":"Installing the charts","text":""},{"location":"started/#git-clone","title":"Git Clone","text":"<p>First, clone the repository to your machine. The precise way to do that depends on your setup, but usually: <code>git clone https://github.com/UnfamousThomas/thesis-initial</code></p>"},{"location":"started/#installing-the-crds-chart","title":"Installing the CRDs chart","text":"<p>Installing the custom resources chart is quite straightforward, simply navigate to: <code>operator/charts/</code> and then run  <pre><code>helm install thesis-crds ./thesis-crds\n</code></pre> This will setup the crds on the cluster.</p>"},{"location":"started/#installing-the-manager-chart","title":"Installing the manager chart","text":"<p>Installing the manager is a little bit more complicated, as you need to make sure the values are correct first. First, navigate to: <code>operator/charts/thesis-operator</code>. </p>"},{"location":"started/#valuesyaml","title":"Values.yaml","text":"<p>This folder has a <code>values.yaml</code> file, with all the files, it should look like this:</p> <pre><code>controllerManager:\n  manager:\n    args:\n    - --metrics-bind-address=:8443\n    - --leader-elect\n    - --health-probe-bind-address=:8081\n    containerSecurityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n    env:\n      imagePullSecretName: ghcr-secret\n    image:\n      repository: ghcr.io/unfamousthomas/controller\n      tag: latest\n    resources:\n      limits:\n        cpu: 500m\n        memory: 128Mi\n      requests:\n        cpu: 10m\n        memory: 64Mi\n  replicas: 1\n  serviceAccount:\n    annotations: {}\nkubernetesClusterDomain: cluster.local\nmetricsService:\n  ports:\n  - name: https\n    port: 8443\n    protocol: TCP\n    targetPort: 8443\n  type: ClusterIP\nwebhookService:\n  ports:\n  - port: 443\n    protocol: TCP\n    targetPort: 9443\n  type: ClusterIP\nservice:\n  enabled: true\n</code></pre> <p>The important parts of this are the following fields: <code>controllerManager.manager.env.imagePullSecretName</code> and <code>service.enabled</code>.</p> <p>The <code>controllerManager.manager.env.imagePullSecretName</code> field should be set to be what you created the github access token secret as. This is used to pull the sidecar image.</p> <p>The <code>service.enabled</code> determines whether or not we deploy the Service to the cluster. You can read about that in Service.</p>"},{"location":"started/#installing-the-chart","title":"Installing the chart","text":"<p>Once you are good with the values.yaml, just run: <pre><code>helm install thesis-manager .\n</code></pre> in the folder.</p>"}]}